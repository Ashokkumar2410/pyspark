{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import BasicProfiler\n",
    "class MyCustomProfiler(BasicProfiler):\n",
    "    def show(self, id):\n",
    "        print(\"My custom profiles for RDD:%s\" % id)\n",
    "\n",
    "conf = SparkConf().set(\"spark.python.profile\", \"true\")\n",
    "sc = SparkContext('local', 'test', conf=conf, profiler_cls=MyCustomProfiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcarsfile = \"../../data/mtcars.csv\"\n",
    "mtcars_rdd = sc.textFile(mtcarsfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AMC Javelin,15.2,8,304,150,3.15,3.435,17.3,0,0,3,2',\n",
       " 'Cadillac Fleetwood,10.4,8,472,205,2.93,5.25,17.98,0,0,3,4',\n",
       " 'Camaro Z28,13.3,8,350,245,3.73,3.84,15.41,0,0,3,4',\n",
       " 'Chrysler Imperial,14.7,8,440,230,3.23,5.345,17.42,0,0,3,4']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_rdd.takeOrdered(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model,mpg,cyl,disp,hp,drat,wt,qsec,vs,am,gear,carb',\n",
       " 'Mazda RX4,21,6,160,110,3.9,2.62,16.46,0,1,4,4',\n",
       " 'Mazda RX4 Wag,21,6,160,110,3.9,2.875,17.02,0,1,4,4',\n",
       " 'Datsun 710,22.8,4,108,93,3.85,2.32,18.61,1,1,4,1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars_rdd_list = mtcars_rdd.map(lambda x:x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars_header = mtcars_rdd_list.first()\n",
    "mtcars_rows = mtcars_rdd_list.filter(lambda x : x != mtcars_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mazda RX4',\n",
       "  '21',\n",
       "  '6',\n",
       "  '160',\n",
       "  '110',\n",
       "  '3.9',\n",
       "  '2.62',\n",
       "  '16.46',\n",
       "  '0',\n",
       "  '1',\n",
       "  '4',\n",
       "  '4']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_rows.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import BasicProfiler\n",
    "class MyCustomProfiler(BasicProfiler):\n",
    "    def show(self, id):\n",
    "        stats = self.stats()\n",
    "        if stats:\n",
    "            print(\"=\" * 60)\n",
    "            print(\"Profile of RDD<id=%d>\" % id)\n",
    "            print(\"=\" * 60)\n",
    "            stats.sort_stats(\"time\", \"cumulative\").print_stats()\n",
    "\n",
    "conf = SparkConf().set(\"spark.python.profile\", \"true\")\n",
    "sc = SparkContext('local', 'test', conf=conf, profiler_cls=MyCustomProfiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mazda RX4',\n",
       "  '21',\n",
       "  '6',\n",
       "  '160',\n",
       "  '110',\n",
       "  '3.9',\n",
       "  '2.62',\n",
       "  '16.46',\n",
       "  '0',\n",
       "  '1',\n",
       "  '4',\n",
       "  '4']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcarsfile = \"../../data/mtcars.csv\"\n",
    "mtcars_rdd = sc.textFile(mtcarsfile)\n",
    "\n",
    "mtcars_rdd_list = mtcars_rdd.map(lambda x:x.split(\",\"))\n",
    "\n",
    "mtcars_header = mtcars_rdd_list.first()\n",
    "mtcars_rows = mtcars_rdd_list.filter(lambda x : x != mtcars_header)\n",
    "\n",
    "mtcars_rows.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Profile of RDD<id=2>\n",
      "============================================================\n",
      "         31 function calls in 0.000 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 serializers.py:389(dump_stream)\n",
      "        1    0.000    0.000    0.000    0.000 worker.py:370(process)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _pickle.dumps}\n",
      "        2    0.000    0.000    0.000    0.000 serializers.py:686(load_stream)\n",
      "        1    0.000    0.000    0.000    0.000 serializers.py:714(read_int)\n",
      "        1    0.000    0.000    0.000    0.000 serializers.py:575(dumps)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        1    0.000    0.000    0.000    0.000 serializers.py:677(loads)\n",
      "        1    0.000    0.000    0.000    0.000 rdd.py:2498(pipeline_func)\n",
      "        2    0.000    0.000    0.000    0.000 rdd.py:1349(takeUpToNumLeft)\n",
      "        1    0.000    0.000    0.000    0.000 serializers.py:721(write_int)\n",
      "        1    0.000    0.000    0.000    0.000 rdd.py:351(func)\n",
      "        1    0.000    0.000    0.000    0.000 <ipython-input-26-d7b4fe9b11e0>:4(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 rdd.py:323(func)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "        1    0.000    0.000    0.000    0.000 util.py:97(wrapper)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 util.py:92(fail_on_stopiteration)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _struct.unpack}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n",
      "============================================================\n",
      "Profile of RDD<id=3>\n",
      "============================================================\n",
      "         49 function calls (48 primitive calls) in 0.000 seconds\n",
      "\n",
      "   Ordered by: internal time, cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.000    0.000 serializers.py:389(dump_stream)\n",
      "        1    0.000    0.000    0.000    0.000 worker.py:370(process)\n",
      "        3    0.000    0.000    0.000    0.000 serializers.py:686(load_stream)\n",
      "        2    0.000    0.000    0.000    0.000 serializers.py:714(read_int)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _pickle.dumps}\n",
      "        2    0.000    0.000    0.000    0.000 serializers.py:677(loads)\n",
      "        1    0.000    0.000    0.000    0.000 serializers.py:575(dumps)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 rdd.py:1349(takeUpToNumLeft)\n",
      "        1    0.000    0.000    0.000    0.000 rdd.py:323(func)\n",
      "        4    0.000    0.000    0.000    0.000 util.py:97(wrapper)\n",
      "        2    0.000    0.000    0.000    0.000 rdd.py:351(func)\n",
      "        1    0.000    0.000    0.000    0.000 serializers.py:721(write_int)\n",
      "      2/1    0.000    0.000    0.000    0.000 rdd.py:2498(pipeline_func)\n",
      "        2    0.000    0.000    0.000    0.000 <ipython-input-26-d7b4fe9b11e0>:4(<lambda>)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'decode' of 'bytes' objects}\n",
      "        2    0.000    0.000    0.000    0.000 <ipython-input-26-d7b4fe9b11e0>:7(<lambda>)\n",
      "        2    0.000    0.000    0.000    0.000 util.py:92(fail_on_stopiteration)\n",
      "        1    0.000    0.000    0.000    0.000 rdd.py:401(func)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'write' of '_io.BufferedWriter' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'read' of '_io.BufferedReader' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method _struct.unpack}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _struct.pack}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.show_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mazda RX4',\n",
       "  '21',\n",
       "  '6',\n",
       "  '160',\n",
       "  '110',\n",
       "  '3.9',\n",
       "  '2.62',\n",
       "  '16.46',\n",
       "  '0',\n",
       "  '1',\n",
       "  '4',\n",
       "  '4'],\n",
       " ['Mazda RX4 Wag',\n",
       "  '21',\n",
       "  '6',\n",
       "  '160',\n",
       "  '110',\n",
       "  '3.9',\n",
       "  '2.875',\n",
       "  '17.02',\n",
       "  '0',\n",
       "  '1',\n",
       "  '4',\n",
       "  '4']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_rows.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars_header[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sparkContext=sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcars_df = spark.createDataFrame(mtcars_rows,mtcars_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+---+----+---+----+-----+-----+---+---+----+----+\n",
      "|        model| mpg|cyl|disp| hp|drat|   wt| qsec| vs| am|gear|carb|\n",
      "+-------------+----+---+----+---+----+-----+-----+---+---+----+----+\n",
      "|    Mazda RX4|  21|  6| 160|110| 3.9| 2.62|16.46|  0|  1|   4|   4|\n",
      "|Mazda RX4 Wag|  21|  6| 160|110| 3.9|2.875|17.02|  0|  1|   4|   4|\n",
      "|   Datsun 710|22.8|  4| 108| 93|3.85| 2.32|18.61|  1|  1|   4|   1|\n",
      "+-------------+----+---+----+---+----+-----+-----+---+---+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mtcars_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.show_profiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanicfile = \"../../data/titanic.tsv\"\n",
    "titanic_df = spark.read.csv(titanicfile,sep=\"\\t\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "|Survived|Pclass|                Name|   Sex|Age|Siblings_Spouses Aboard|Parents_Children Aboard|   Fare|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "|       0|     3|Mr. Owen Harris B...|  male| 22|                      1|                      0|   7.25|\n",
      "|       1|     1|Mrs. John Bradley...|female| 38|                      1|                      0|71.2833|\n",
      "|       1|     3|Miss. Laina Heikk...|female| 26|                      0|                      0|  7.925|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"myapp\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some-value'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.some.config.option')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_checkType',\n",
       " '_jconf',\n",
       " 'get',\n",
       " 'isModifiable',\n",
       " 'set',\n",
       " 'unset']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(spark.conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "l =sc.parallelize([('name','ashok'),('name','kumar')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'ashok'), ('name', 'kumar')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'kumar'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|  _1|   _2|\n",
      "+----+-----+\n",
      "|name|ashok|\n",
      "|name|kumar|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(l).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "l =[(\"ashok\",\"kumar\",\"29\"),((\"Rama\",\"krishnan\",\"59\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "Person = Row('firstname', 'lastname','age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ashok', 'kumar', '29'), ('Rama', 'krishnan', '59')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(l)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = rdd.map(lambda x : Person(*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(firstname='ashok', lastname='kumar', age='29'),\n",
       " Row(firstname='Rama', lastname='krishnan', age='59')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(v):\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ashok', 'kumar', '29')\n"
     ]
    }
   ],
   "source": [
    "print(s(l[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ashok kumar 29\n"
     ]
    }
   ],
   "source": [
    "print(*l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "   StructField(\"firstname\", StringType(), True),\n",
    "   StructField(\"lastname\", StringType(), True),\n",
    "   StructField(\"age\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "l =[(\"ashok\",\"kumar\",29),((\"Rama\",\"krishnan\",59))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD = sc.parallelize(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(RDD,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(firstname='ashok', lastname='kumar', age=29),\n",
       " Row(firstname='Rama', lastname='krishnan', age=59)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---+\n",
      "|firstname|lastname|age|\n",
      "+---------+--------+---+\n",
      "|    ashok|   kumar| 29|\n",
      "|     Rama|krishnan| 59|\n",
      "+---------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a1='ashok', a2='kumar', b=29), Row(a1='Rama', a2='krishnan', b=59)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(RDD, \"a1: string, a2:string, b: int\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sql = titanic_df.createOrReplaceTempView(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sql = spark.sql(\"SELECT * FROM titanic LIMIT 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "|Survived|Pclass|                Name|   Sex|Age|Siblings_Spouses Aboard|Parents_Children Aboard|   Fare|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "|       0|     3|Mr. Owen Harris B...|  male| 22|                      1|                      0|   7.25|\n",
      "|       1|     1|Mrs. John Bradley...|female| 38|                      1|                      0|71.2833|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+-------+\n",
      "|Survived|Pclass|   Sex|Numbers|\n",
      "+--------+------+------+-------+\n",
      "|       0|     1|female|      3|\n",
      "|       1|     1|female|     91|\n",
      "|       0|     2|female|      6|\n",
      "|       1|     2|female|     70|\n",
      "|       0|     3|female|     72|\n",
      "|       1|     3|female|     72|\n",
      "|       1|     1|  male|     45|\n",
      "|       0|     1|  male|     77|\n",
      "|       1|     2|  male|     17|\n",
      "|       0|     2|  male|     91|\n",
      "|       1|     3|  male|     47|\n",
      "|       0|     3|  male|    296|\n",
      "+--------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Survived,Pclass,Sex,Count(*) AS Numbers FROM titanic GROUP BY Pclass,Survived,Sex ORDER BY SEX,Pclass,Numbers\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df=titanic_df, tableName=\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Survived='0', Pclass='3', Name='Mr. Owen Harris Braund', Sex='male', Age='22', Siblings_Spouses Aboard='1', Parents_Children Aboard='0', Fare='7.25'),\n",
       " Row(Survived='1', Pclass='1', Name='Mrs. John Bradley (Florence Briggs Thayer) Cumings', Sex='female', Age='38', Siblings_Spouses Aboard='1', Parents_Children Aboard='0', Fare='71.2833')]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.table(\"titanic\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "jsondir=\"../../data/\"\n",
    "people_df   = spark.read.json(os.path.join(jsondir,'people.json')) \n",
    "employee_df = spark.read.json(os.path.join(jsondir,'employee.json')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df.write.parquet(\"../../data/people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_df.write.parquet(\"../../data/employee.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "people   = spark.read.parquet(\"../../data/people.parquet\")\n",
    "employee = spark.read.parquet(\"../../data/employee.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, name='Justin', name='Justin', salary=3500)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.filter(people.age < 30).join(employee,  people.name==employee.name ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_people_employee = people.filter(people.age < 30).join(employee,  people.name==employee.name ).groupBy(people.name, \"age\").agg({\"salary\": \"avg\", \"age\": \"max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-----------+--------+\n",
      "|  name|age|avg(salary)|max(age)|\n",
      "+------+---+-----------+--------+\n",
      "|Justin| 19|     3500.0|      19|\n",
      "+------+---+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_people_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.createGlobalTempView(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.people\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Temporary view 'people' already exists;\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    people.createGlobalTempView(\"people\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    ## AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "| age|   name|salary|\n",
      "+----+-------+------+\n",
      "|null|Michael|  3000|\n",
      "|null|Michael|  4500|\n",
      "|null|Michael|  3500|\n",
      "|null|Michael|  4000|\n",
      "|  30|   Andy|  3000|\n",
      "|  30|   Andy|  4500|\n",
      "|  30|   Andy|  3500|\n",
      "|  30|   Andy|  4000|\n",
      "|  19| Justin|  3000|\n",
      "|  19| Justin|  4500|\n",
      "|  19| Justin|  3500|\n",
      "|  19| Justin|  4000|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.crossJoin(employee.select(\"salary\")).select(\"age\", \"name\",\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|salary|\n",
      "+-------+----+------+\n",
      "|Michael|null|  3000|\n",
      "|   Andy|  30|  4500|\n",
      "| Justin|  19|  3500|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.join(employee,  ['name'] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|   name| age|salary|\n",
      "+-------+----+------+\n",
      "|Michael|null|  3000|\n",
      "|   Andy|  30|  4500|\n",
      "|  Berta|null|  4000|\n",
      "| Justin|  19|  3500|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.join(employee,on = \"name\" , how='outer').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+------+\n",
      "|   name| age|salary| Basic|\n",
      "+-------+----+------+------+\n",
      "|Michael|null|  3000|1500.0|\n",
      "|   Andy|  30|  4500|2250.0|\n",
      "|  Berta|null|  4000|2000.0|\n",
      "| Justin|  19|  3500|1750.0|\n",
      "+-------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.join(employee,on = \"name\" , how='outer').withColumn(\"Basic\", employee.salary/2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=None, name='Michael'),\n",
       " Row(age=30, name='Andy'),\n",
       " Row(age=19, name='Justin')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table(\"global_temp.people\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropGlobalTempView(\"global_temp.people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table already deleted :\"Table or view not found: `global_temp`.`my_table`;;\\n'UnresolvedRelation `global_temp`.`my_table`\\n\"\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.table(\"global_temp.my_table\") \n",
    "except Exception as e:\n",
    "    print(\"\"\"Table already deleted :%s\"\"\" %(e)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.listDatabases\n",
    "except AttributeError:\n",
    "    print(\"Exception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dt='2015-04-08', current=datetime.datetime(2020, 3, 30, 12, 3, 15, 763000))]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"current\",current_timestamp()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        dt|   current|\n",
      "+----------+----------+\n",
      "|2015-04-08|2020-03-30|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"current\",current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|        dt|next_month|\n",
      "+----------+----------+\n",
      "|2015-04-08|2015-05-08|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"next_month\",date_add(df.dt,30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+\n",
      "|        dt|next_month|Prev_month|\n",
      "+----------+----------+----------+\n",
      "|2015-04-08|2015-05-08|2015-03-09|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"next_month\",date_add(df.dt,30)).withColumn(\"Prev_month\",date_sub(df.dt, 30)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### timestamp truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('2019-12-28 05:02:11',)], ['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|date_trunc(year, t)|\n",
      "+-------------------+\n",
      "|2019-01-01 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(date_trunc('year',df.t)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date_trunc(year, t)=datetime.datetime(2019, 1, 1, 0, 0))]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(date_trunc('year',df.t)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| date_trunc(mon, t)|\n",
      "+-------------------+\n",
      "|2019-12-01 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(date_trunc('mon',df.t)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|                  t|              month|\n",
      "+-------------------+-------------------+\n",
      "|2019-12-28 05:02:11|2019-12-01 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"month\",date_trunc('mon',df.t) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|diff|\n",
      "+----+\n",
      "|  32|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
    "df.select(datediff(df.d2, df.d1).alias('diff')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day=98)]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(dayofyear('dt').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day=4)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(dayofweek('dt').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(day=8)]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
    "df.select(dayofmonth('dt').alias('day')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+\n",
      "|  a|  intlist|mapfield|\n",
      "+---+---------+--------+\n",
      "|  1|[1, 2, 3]|[a -> b]|\n",
      "+---+---------+--------+\n",
      "\n",
      "+-----+\n",
      "|anInt|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.show()\n",
    "eDF.select(explode(eDF.intlist).alias(\"anInt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  a|    b|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eDF.select(explode(eDF.mapfield).alias(\"key\",\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import IntegerType, StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "   Scalar UDFs are used with :meth:`pyspark.sql.DataFrame.withColumn` and\n",
    "   :meth:`pyspark.sql.DataFrame.select`.\n",
    "\n",
    "   >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "   >>> from pyspark.sql.types import IntegerType, StringType\n",
    "   >>> slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP\n",
    "   >>> @pandas_udf(StringType())  # doctest: +SKIP\n",
    "   ... def to_upper(s):\n",
    "   ...     return s.str.upper()\n",
    "   ...\n",
    "   >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n",
    "   ... def add_one(x):\n",
    "   ...     return x + 1\n",
    "   ...\n",
    "   >>> df = spark.createDataFrame([(1, \"John Doe\", 21)],\n",
    "   ...                            (\"id\", \"name\", \"age\"))  # doctest: +SKIP\n",
    "   >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")) \\\n",
    "   ...     .show()  # doctest: +SKIP\n",
    "   +----------+--------------+------------+\n",
    "   |slen(name)|to_upper(name)|add_one(age)|\n",
    "   +----------+--------------+------------+\n",
    "   |         8|      JOHN DOE|          22|\n",
    "   +----------+--------------+------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dt='2015-04-08', randn=0.4085363219031828)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('randn', randn(seed=42)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dt='2015-04-08', randn=1.011340411856771)]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('randn', randn()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "|Survived|Pclass|                Name|   Sex|Age|Siblings_Spouses Aboard|Parents_Children Aboard|   Fare|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "|       0|     3|Mr. Owen Harris B...|  male| 22|                      1|                      0|   7.25|\n",
      "|       1|     1|Mrs. John Bradley...|female| 38|                      1|                      0|71.2833|\n",
      "|       1|     3|Miss. Laina Heikk...|female| 26|                      0|                      0|  7.925|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+-----+\n",
      "|Survived|Pclass|                Name|   Sex|Age|Siblings_Spouses Aboard|Parents_Children Aboard|   Fare|Title|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+-----+\n",
      "|       0|     3|Mr. Owen Harris B...|  male| 22|                      1|                      0|   7.25|   Mr|\n",
      "|       1|     1|Mrs. John Bradley...|female| 38|                      1|                      0|71.2833|  Mrs|\n",
      "|       1|     3|Miss. Laina Heikk...|female| 26|                      0|                      0|  7.925| Miss|\n",
      "+--------+------+--------------------+------+---+-----------------------+-----------------------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_df.withColumn(\"Title\",regexp_extract(titanic_df.Name, r'(\\w+)?\\.', 1).alias('title')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(d='-----')]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([('100-200',)], ['str'])\n",
    "df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "887"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.createOrReplaceTempView(\"titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+------------------------+\n",
      "|Pclass|Survived|   Sex|avg(CAST(age AS DOUBLE))|\n",
      "+------+--------+------+------------------------+\n",
      "|     1|       0|female|      25.666666666666668|\n",
      "|     1|       1|female|       35.57142857142857|\n",
      "|     1|       0|  male|       44.29220779220779|\n",
      "|     1|       1|  male|       36.75377777777778|\n",
      "|     2|       0|female|                    36.0|\n",
      "|     2|       1|  male|      17.078235294117647|\n",
      "|     2|       0|  male|                    33.0|\n",
      "|     2|       1|female|       28.37857142857143|\n",
      "|     3|       1|  male|      22.242978723404256|\n",
      "|     3|       0|  male|       27.14189189189189|\n",
      "|     3|       1|female|      20.868055555555557|\n",
      "|     3|       0|female|       23.40277777777778|\n",
      "+------+--------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Pclass,Survived,Sex,AVG(age) FROM titanic GROUP BY Pclass,Sex,Survived ORDER BY Pclass \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT Pclass,Survived,Sex,AVG(age) FROM titanic GROUP BY Pclass,Sex,Survived ORDER BY Pclass \").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
